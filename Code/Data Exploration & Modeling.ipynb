{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section Order:\n",
    "(1) Import Packages & Data\n",
    "\n",
    "(2) Getting acquanted with the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Packages & Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Data Basics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt # side-stepping mpl backend\n",
    "from pandas.tools.plotting import scatter_matrix\n",
    "%matplotlib inline\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn import preprocessing as pp\n",
    "\n",
    "# Regression / Modeling\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Custom functions\n",
    "from project_functions import label_polynomial_features\n",
    "from project_functions import model_to_dictionary\n",
    "from project_functions import results_summary_to_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_raw = pd.read_csv('../Resources/Data/Raw/mypersonality_final.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting acquanted with the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>#AUTHID</th>\n",
       "      <td>b7b7764cfa1c523e4e93ab2a79a946c4</td>\n",
       "      <td>b7b7764cfa1c523e4e93ab2a79a946c4</td>\n",
       "      <td>b7b7764cfa1c523e4e93ab2a79a946c4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STATUS</th>\n",
       "      <td>likes the sound of thunder.</td>\n",
       "      <td>is so sleepy it's not even funny that's she ca...</td>\n",
       "      <td>is sore and wants the knot of muscles at the b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sEXT</th>\n",
       "      <td>2.65</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sNEU</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sAGR</th>\n",
       "      <td>3.15</td>\n",
       "      <td>3.15</td>\n",
       "      <td>3.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sCON</th>\n",
       "      <td>3.25</td>\n",
       "      <td>3.25</td>\n",
       "      <td>3.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sOPN</th>\n",
       "      <td>4.4</td>\n",
       "      <td>4.4</td>\n",
       "      <td>4.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cEXT</th>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cNEU</th>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cAGR</th>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cCON</th>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cOPN</th>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DATE</th>\n",
       "      <td>06/19/09 03:21 PM</td>\n",
       "      <td>07/02/09 08:41 AM</td>\n",
       "      <td>06/15/09 01:15 PM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NETWORKSIZE</th>\n",
       "      <td>180</td>\n",
       "      <td>180</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BETWEENNESS</th>\n",
       "      <td>14861.6</td>\n",
       "      <td>14861.6</td>\n",
       "      <td>14861.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NBETWEENNESS</th>\n",
       "      <td>93.29</td>\n",
       "      <td>93.29</td>\n",
       "      <td>93.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DENSITY</th>\n",
       "      <td>0.03</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BROKERAGE</th>\n",
       "      <td>15661</td>\n",
       "      <td>15661</td>\n",
       "      <td>15661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NBROKERAGE</th>\n",
       "      <td>0.49</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TRANSITIVITY</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             0  \\\n",
       "#AUTHID       b7b7764cfa1c523e4e93ab2a79a946c4   \n",
       "STATUS             likes the sound of thunder.   \n",
       "sEXT                                      2.65   \n",
       "sNEU                                         3   \n",
       "sAGR                                      3.15   \n",
       "sCON                                      3.25   \n",
       "sOPN                                       4.4   \n",
       "cEXT                                         n   \n",
       "cNEU                                         y   \n",
       "cAGR                                         n   \n",
       "cCON                                         n   \n",
       "cOPN                                         y   \n",
       "DATE                         06/19/09 03:21 PM   \n",
       "NETWORKSIZE                                180   \n",
       "BETWEENNESS                            14861.6   \n",
       "NBETWEENNESS                             93.29   \n",
       "DENSITY                                   0.03   \n",
       "BROKERAGE                                15661   \n",
       "NBROKERAGE                                0.49   \n",
       "TRANSITIVITY                               0.1   \n",
       "\n",
       "                                                              1  \\\n",
       "#AUTHID                        b7b7764cfa1c523e4e93ab2a79a946c4   \n",
       "STATUS        is so sleepy it's not even funny that's she ca...   \n",
       "sEXT                                                       2.65   \n",
       "sNEU                                                          3   \n",
       "sAGR                                                       3.15   \n",
       "sCON                                                       3.25   \n",
       "sOPN                                                        4.4   \n",
       "cEXT                                                          n   \n",
       "cNEU                                                          y   \n",
       "cAGR                                                          n   \n",
       "cCON                                                          n   \n",
       "cOPN                                                          y   \n",
       "DATE                                          07/02/09 08:41 AM   \n",
       "NETWORKSIZE                                                 180   \n",
       "BETWEENNESS                                             14861.6   \n",
       "NBETWEENNESS                                              93.29   \n",
       "DENSITY                                                    0.03   \n",
       "BROKERAGE                                                 15661   \n",
       "NBROKERAGE                                                 0.49   \n",
       "TRANSITIVITY                                                0.1   \n",
       "\n",
       "                                                              2  \n",
       "#AUTHID                        b7b7764cfa1c523e4e93ab2a79a946c4  \n",
       "STATUS        is sore and wants the knot of muscles at the b...  \n",
       "sEXT                                                       2.65  \n",
       "sNEU                                                          3  \n",
       "sAGR                                                       3.15  \n",
       "sCON                                                       3.25  \n",
       "sOPN                                                        4.4  \n",
       "cEXT                                                          n  \n",
       "cNEU                                                          y  \n",
       "cAGR                                                          n  \n",
       "cCON                                                          n  \n",
       "cOPN                                                          y  \n",
       "DATE                                          06/15/09 01:15 PM  \n",
       "NETWORKSIZE                                                 180  \n",
       "BETWEENNESS                                             14861.6  \n",
       "NBETWEENNESS                                              93.29  \n",
       "DENSITY                                                    0.03  \n",
       "BROKERAGE                                                 15661  \n",
       "NBROKERAGE                                                 0.49  \n",
       "TRANSITIVITY                                                0.1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw.head(3).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_grouped = df_raw.groupby(by = \"#AUTHID\", as_index = False).first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_grouped[['sEXT', 'sNEU', 'sAGR', 'sCON', 'sOPN',\"NETWORKSIZE\",\"BETWEENNESS\"]].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_grouped_reduced = df_grouped[['sEXT', 'sNEU', 'sAGR', 'sCON', 'sOPN',\"NETWORKSIZE\",\"BETWEENNESS\"]].sort_values(by= \"BETWEENNESS\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_grouped.quantile([0.1,0.25,0.5,0.75,0.9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_grouped[\"BETWEENNESS\"].quantile(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_grouped[\"BETWEENNESS\"].quantile(0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Give me all the people who are in the bottom 10% in Betweenness Centrality\n",
    "low_users_df = df_grouped_reduced[df_grouped_reduced[\"BETWEENNESS\"]<df_grouped[\"BETWEENNESS\"].quantile(0.1)]\n",
    "average_low_user = df_grouped_reduced[df_grouped_reduced[\"BETWEENNESS\"]<df_grouped[\"BETWEENNESS\"].quantile(0.1)].mean()\n",
    "\n",
    "# Fairly high openness... intelligence leads to a decrease in working in grous?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Give me all the people who are in the top 10% in Betweenness Centrality\n",
    "high_users_df = df_grouped_reduced[df_grouped_reduced[\"BETWEENNESS\"]>df_grouped[\"BETWEENNESS\"].quantile(0.9)]\n",
    "average_high_user = df_grouped_reduced[df_grouped_reduced[\"BETWEENNESS\"]<df_grouped[\"BETWEENNESS\"].quantile(0.9)].mean()\n",
    "\n",
    "# Fairly high openness... intelligence leads to a decrease in working in grous?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Low User:\"\n",
    "print average_low_user\n",
    "print\n",
    "print \"High User:\"\n",
    "print average_high_user\n",
    "print\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gl.canvas.set_target('ipynb')\n",
    "\n",
    "\n",
    "high_users_sl = gl.SFrame(high_users_df)\n",
    "low_users_sl = gl.SFrame(low_users_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation: High users clearly \"all\" are oriented toward greater extroversion. \n",
    "Low users are more spread out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "high_users_sl[\"sEXT\"].show()\n",
    "low_users_sl[\"sEXT\"].show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation: High Users show greater diversity of values in consciousness. Whereas low users tend to be centered in the middle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "high_users_sl[\"sCON\"].show()\n",
    "low_users_sl[\"sCON\"].show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# High users tend to have low values of neuroticism . Low users are more spread out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "high_users_sl[\"sNEU\"].show()\n",
    "low_users_sl[\"sNEU\"].show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the same in both groups.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "high_users_sl[\"sAGR\"].show()\n",
    "low_users_sl[\"sAGR\"].show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# High users tend to have/ be focused on the extreme positive. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "high_users_sl[\"sOPN\"].show()\n",
    "low_users_sl[\"sOPN\"].show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting some time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_extroversion = df_grouped[\"sEXT\"]\n",
    "x_neuroticism = df_grouped[\"sNEU\"]\n",
    "x_agreeableness = df_grouped[\"sAGR\"]\n",
    "x_conscientiousness = df_grouped[\"sCON\"]\n",
    "x_openness = df_grouped[\"sOPN\"]\n",
    "\n",
    "# y_betweenness = df_grouped[\"BETWEENNESS\"] / df_grouped[\"NETWORKSIZE\"]   # Temporary\n",
    "y_betweenness = df_grouped[\"BETWEENNESS\"]\n",
    "\n",
    "\n",
    "y_nbetweenness = df_grouped[\"NBETWEENNESS\"]\n",
    "\n",
    "network_size = df_grouped[\"NETWORKSIZE\"]\n",
    "\n",
    "variable_list = [x_extroversion,x_neuroticism,x_agreeableness,x_conscientiousness,x_openness]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List of Basic Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_big5 = pd.DataFrame([x_extroversion,x_neuroticism,x_agreeableness,x_conscientiousness,x_openness]).T\n",
    "X_network = pd.DataFrame([network_size]).T\n",
    "X_big5_and_network = pd.DataFrame([x_extroversion,x_neuroticism,x_agreeableness,x_conscientiousness,x_openness,network_size]).T\n",
    "X_big5_and_network_betweenness = pd.DataFrame([x_extroversion,x_neuroticism,x_agreeableness,x_conscientiousness,x_openness,network_size,y_betweenness]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating some useful scatterplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scatter_matrix(X_big5_and_network_betweenness, alpha=0.2, figsize=(15,15), diagonal='kde')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a function which does a \"polynomial expansion\" for all the individual features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate expanded models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_big5.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing out alternate solution to preprocessing code - Begining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_big5_test = X_big5\n",
    "X_big5_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "poly = pp.PolynomialFeatures(2,include_bias=True)\n",
    "output_nparray = poly.fit_transform(X_big5_test)\n",
    "powers_nparray = poly.powers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target_feature_names = ['x'.join(['{}^{}'.format(pair[0],pair[1]) for pair in tuple if pair[1]!=0]) for tuple in [zip(X_big5_test.columns,p) for p in poly.powers_]]\n",
    "output_df = pd.DataFrame(output_nparray, columns = target_feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## It works but doesn't add in the constant term. Regardless, a lot faster it seems than how i did it. WIll need to digest further later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing out alternate solution to preprocessing code - End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# THis has to come first otherwise the constant term will get duplicated\n",
    "X_big5_polyexpanded = label_polynomial_features(X_big5,2,bias = True)\n",
    "X_network_polyexpanded =  label_polynomial_features(X_network,2,bias = True)\n",
    "X_big5_and_network_polyexpanded =  label_polynomial_features(X_big5_and_network,2,bias = True)\n",
    "\n",
    "# The reason I'm recreating these with polynomial expansion is just so it will have consistent feature names with the others\n",
    "X_big5= label_polynomial_features(X_big5,1,bias = True)\n",
    "X_network=  label_polynomial_features(X_network,1,bias = True)\n",
    "X_big5_and_network =  label_polynomial_features(X_big5_and_network,1,bias = True)\n",
    "\n",
    "\n",
    "#target_feature_names = ['x'.join(['{}^{}'.format(pair[0],pair[1]) for pair in tuple if pair[1]!=0]) for tuple in [zip(X_big5`.columns,p) for p in poly.powers_]]\n",
    "#output_df = pd.DataFrame(output_nparray, columns = target_feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_of_model_names = [\n",
    "                        'X_big5',\n",
    "                        'X_big5_polyexpanded',\n",
    "                        'X_network',\n",
    "                        'X_network_polyexpanded',\n",
    "                        'X_big5_and_network',\n",
    "                        'X_big5_and_network_polyexpanded']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_big5_polyexpanded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_big5_polyexpanded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This is a dictionary of the models. The keys are the names of the models and the values are the dataframe / \n",
    "#X matricies appropriate to them\n",
    "\n",
    "dictionary_of_models_x_matricies = {\"X_big5\":X_big5,\n",
    "                        \"X_big5_polyexpanded\":X_big5_polyexpanded,\n",
    "                        \"X_network\":X_network,\n",
    "                        \"X_network_polyexpanded\":X_network_polyexpanded,\n",
    "                        \"X_big5_and_network\":X_big5_and_network,\n",
    "                        \"X_big5_and_network_polyexpanded\":X_big5_and_network_polyexpanded,\n",
    "                       }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Data as Necesesary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary Statistics & Descriptive Visualizations (Pandas and Matplotlib, etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot network size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing the covaraiance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_big5_and_network.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.cov(np.vstack([x_extroversion,x_neuroticism,x_agreeableness,x_conscientiousness,x_openness,network_size]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Scatter plots (y vs. Xi) for each of the X's -- this suggest relationship"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check features for normality by using a density plot - plot normal on top of it. If it's not normal, then do some transformations on the features until they get to be normal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing (Regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1a: (via intuition) Generate different models and provide explanations of why (avoid products, okay for squares and logs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1b: Algorithmly - go throuhg all the options and create them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Separate Data into 3 groupings: training, cv, and \n",
    "- Create seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_of_all_possible_features = list(dictionary_of_models_x_matricies[\"X_big5_and_network_polyexpanded\"].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    ".7*250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dictionary_of_model_general_performance_metrics = {}\n",
    "for model_name,model_matrix in dictionary_of_models_x_matricies.items():\n",
    "    print model_name\n",
    "    X_train, X_test, y_train, y_test = cross_validation.train_test_split(model_matrix, y_betweenness, test_size=0.3, random_state=0)\n",
    "    model = sm.OLS(y_train,X_train)\n",
    "    dictionary_of_model_general_performance_metrics[model_name] = model_to_dictionary(model,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dictionary_of_model_general_performance_metrics[\"X_big5\"]['Z_Test RSqAdj']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Using the Training set, determine the best coefficients for each model... (Fitting a model -- sm.fit()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_names = []\n",
    "number_regressors = []\n",
    "features_names = []\n",
    "train_rsq_adj = []\n",
    "train_ssr = []\n",
    "test_rsq_adj = []\n",
    "test_sse = []\n",
    "z_test_rsq_adj = []\n",
    "\n",
    "z_test_sst = []\n",
    "#model_dict[\"test_sst\"] = test_sst\n",
    "\n",
    "\n",
    "for model in dictionary_of_model_general_performance_metrics.keys():\n",
    "    model_names.append(model)\n",
    "    number_regressors.append(dictionary_of_model_general_performance_metrics[model][\"Number Regressors\"])\n",
    "    \n",
    "    string_of_features = \"; \".join(dictionary_of_model_general_performance_metrics[model][\"Feature Names\"])  # This converts a list of feature names into a single string containing all the names\n",
    "    features_names.append(string_of_features)\n",
    "    train_rsq_adj.append(dictionary_of_model_general_performance_metrics[model][\"TRAIN RSqAdj\"])\n",
    "    train_ssr.append(dictionary_of_model_general_performance_metrics[model][\"TRAIN SSR\"])\n",
    "    test_rsq_adj.append(dictionary_of_model_general_performance_metrics[model][\"Test RSqAdj\"])\n",
    "    test_sse.append(dictionary_of_model_general_performance_metrics[model][\"Test SSE\"])\n",
    "    z_test_rsq_adj.append(dictionary_of_model_general_performance_metrics[model][\"Z_Test RSqAdj\"])   #Added for troubleshooting purposes\n",
    "    z_test_sst.append(dictionary_of_model_general_performance_metrics[model][\"z_test_sst\"])    #Added for troubleshooting purposes\n",
    "\n",
    "names = [\"model_names\",\"number_regressors\",\"feature_names\",\"train_rsq_adj\",\"train_ssr\",\"test_rsq_adj\",\"z_test_rsq_adj\",\"test_sse\",\"z_test_sst\"]\n",
    "values = [model_names,number_regressors,features_names,train_rsq_adj,train_ssr,test_rsq_adj,z_test_rsq_adj,test_sse,z_test_sst]\n",
    "\n",
    "blah = pd.DataFrame(dict(zip(names,values)), index = None)\n",
    "blah = blah[[\"model_names\",\"number_regressors\",\"feature_names\",\"train_rsq_adj\",\"train_ssr\",\"test_rsq_adj\",\"z_test_rsq_adj\",\"test_sse\",\"z_test_sst\"]]   #Fixing the order of the columns\n",
    "blah"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ALthough my Z_test_rsq_adj doesn't have negative values and in general the values are smaller than jeremy's...there is still a discrepency, there is likely a problem here that needs to be addressed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dictionary_of_model_general_performance_metrics[\"X_big5\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This thing takes the final metrics from dictionary of model performance, extracts out the Coefficients and \n",
    "# P values for each model. It packages them up into a tuple, reofrmates them into lists, and then they get converted into \n",
    "# a pandas dataframe for \"easy viewing\"\n",
    "\n",
    "dictionary_of_model_coefficients = {}\n",
    "dictionary_of_model_pvalues = {}\n",
    "\n",
    "for feature in list_of_all_possible_features:\n",
    "    #model_name = X_big5\n",
    "    for model_name,model_performance_metrics_dict in dictionary_of_model_general_performance_metrics.items():\n",
    "        #print model_name\n",
    "        #print model_performance_metrics\n",
    "        lookup_key = (feature,model_name)\n",
    "        #print lookup_key\n",
    "        #temp = dictionary_of_model_general_performance_metrics[model_name][\"Feature Names\"] + [\"Constant Term\"]\n",
    "        #print temp\n",
    "        if feature in dictionary_of_model_general_performance_metrics[model_name][\"Feature Names\"]:\n",
    "            p_value_for_feature = model_performance_metrics_dict[\"TRAIN PValues\"][feature]\n",
    "            coef_for_feature = model_performance_metrics_dict[\"Estimated Coefficients\"][feature]\n",
    "        else:\n",
    "            p_value_for_feature = None\n",
    "            coef_for_feature = None            \n",
    "        \n",
    "        dictionary_of_model_coefficients[lookup_key] = coef_for_feature\n",
    "        dictionary_of_model_pvalues[lookup_key] = p_value_for_feature\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "beta_big5 = []\n",
    "pvalues_big5 = []\n",
    "beta_big5_polyexpanded = []\n",
    "pvalues_big5_polyexpanded = []\n",
    "beta_network = []\n",
    "pvalues_network = []\n",
    "beta_network_polyexpanded = []\n",
    "pvalues_network_polyexpanded = []\n",
    "beta_big5_and_network = []\n",
    "pvalues_big5_and_network = []\n",
    "beta_big5_and_network_polyexpanded = []\n",
    "pvalues_big5_and_network_polyexpanded     = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for feature in list_of_all_possible_features:\n",
    "    beta_big5.append(dictionary_of_model_coefficients[(feature,\"X_big5\")])\n",
    "    pvalues_big5.append(dictionary_of_model_pvalues[(feature,\"X_big5\")])\n",
    "    beta_big5_polyexpanded.append(dictionary_of_model_coefficients[(feature,\"X_big5_polyexpanded\")])\n",
    "    pvalues_big5_polyexpanded.append(dictionary_of_model_pvalues[(feature,\"X_big5_polyexpanded\")])\n",
    "    beta_network.append(dictionary_of_model_coefficients[(feature,\"X_network\")])\n",
    "    pvalues_network.append(dictionary_of_model_pvalues[(feature,\"X_network\")])\n",
    "    beta_network_polyexpanded.append(dictionary_of_model_coefficients[(feature,\"X_network_polyexpanded\")])\n",
    "    pvalues_network_polyexpanded.append(dictionary_of_model_pvalues[(feature,\"X_network_polyexpanded\")])\n",
    "    beta_big5_and_network.append(dictionary_of_model_coefficients[(feature,\"X_big5_and_network\")])\n",
    "    pvalues_big5_and_network.append(dictionary_of_model_pvalues[(feature,\"X_big5_and_network\")])\n",
    "    beta_big5_and_network_polyexpanded.append(dictionary_of_model_coefficients[(feature,\"X_big5_and_network_polyexpanded\")])\n",
    "    pvalues_big5_and_network_polyexpanded.append(dictionary_of_model_pvalues[(feature,\"X_big5_and_network_polyexpanded\")])\n",
    "\n",
    "    \n",
    "twinkie = pd.DataFrame({\"feature\":list_of_all_possible_features,\n",
    "                        \"beta_big5\":beta_big5,\n",
    "                        \"pvalues_big5\":pvalues_big5,\n",
    "                        \"beta_big5_polyexpanded\":beta_big5_polyexpanded,\n",
    "                        \"pvalues_big5_polyexpanded\":pvalues_big5_polyexpanded,\n",
    "                        \"beta_network\":beta_network,\n",
    "                        \"pvalues_network\":pvalues_network,\n",
    "                        \"beta_network_polyexpanded\":beta_network_polyexpanded,\n",
    "                        \"pvalues_network_polyexpanded\":pvalues_network_polyexpanded,\n",
    "                        \"beta_big5_and_network\":beta_big5_and_network,\n",
    "                        \"pvalues_big5_and_network\":pvalues_big5_and_network,\n",
    "                        \"beta_big5_and_network_polyexpanded\":beta_big5_and_network_polyexpanded,\n",
    "                        \"pvalues_big5_and_network_polyexpanded\":pvalues_big5_and_network_polyexpanded})\n",
    "                                                                        \n",
    "twinkie2 = twinkie.set_index(\"feature\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print list(twinkie2.columns)\n",
    "twinkie2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "twinkie3 = twinkie2[[\"beta_big5\",\"pvalues_big5\",\"beta_network\",\"pvalues_network\",\"beta_big5_and_network\",\"pvalues_big5_and_network\",\"beta_big5\",\"pvalues_big5\",\"beta_network\",\"pvalues_network\",\"beta_big5_and_network\",\"pvalues_big5_and_network\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "twinkie3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "blah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clearly there's a big need to run lasso on the biggest model! -- Radj is too big and in some cases negative (??)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import cross_validation\n",
    "import statsmodels.api as sm\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.linear_model import Lasso\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_big5_and_network_polyexpanded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_big5_and_network_polyexpanded.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type(np.array(X_big5_and_network_polyexpanded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kf = KFold(n = 250, n_folds=5)\n",
    "\n",
    "#, shuffle=True,random_state=0)\n",
    "# for x in kf:\n",
    "#     print x\n",
    "    \n",
    "lambda_range = np.arange(10990,10995,0.01)    # Ideal to start with a course grid and then make it finer. E.g. (0,1,0.05) -> (0,0.05, 0.001)\n",
    "#lambda_range = np.arange(0,0.033,0.0001)    # Ideal to start with a course grid and then make it finer. E.g. (0,1,0.05) -> (0,0.05, 0.001)\n",
    "\n",
    "error_per_lambda = []\n",
    "#print lambda_range\n",
    "\n",
    "for lambda_value in lambda_range:\n",
    "    errors_per_fold_list = []\n",
    "    for train_index, test_index in kf:\n",
    "        #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "        #print(\"%s %s\" % (train_index.shape, test_index.shape))\n",
    "\n",
    "        X_train, X_test = np.array(X_big5_and_network_polyexpanded)[train_index], np.array(X_big5_and_network_polyexpanded)[test_index]\n",
    "        y_train, y_test = np.array(y_betweenness)[train_index], np.array(y_betweenness)[test_index]\n",
    "\n",
    "        # After this step is where we do the fitting and etc...\n",
    "        # For each fold, train on the training sets (X_train & y_train)\n",
    "        model = Lasso(lambda_value)    # The number entered here is the lambda /alpha variable\n",
    "        model.fit(X_train,y_train)\n",
    "\n",
    "        # For each fold, get a cost for the test sets (X_test & y_test)\n",
    "        y_hat = model.predict(X_test)      # this will generate the predictions\n",
    "        errors = y_test - y_hat\n",
    "        squared_errors = errors * errors\n",
    "        sum_squared_errors = sum(squared_errors)\n",
    "   \n",
    "    #   Returns the coefficient of determination R^2 of the prediction.\n",
    "    #   The coefficient R^2 is defined as (1 - u/v), where u is the regression sum of squares ((y_true - y_pred) ** 2).sum() and v is the residual sum of squares ((y_true - y_true.mean()) ** 2).sum(). Best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.\n",
    "\n",
    "        #r_squared = model.score(X_test,y_test)    # R_squared = 1 - (SSE/SST)\n",
    "        #sse_divied_sst = 1 - r_squared\n",
    "\n",
    "        # Store the cost in a separate cost list\n",
    "\n",
    "        errors_per_fold_list.append(sum_squared_errors)\n",
    "        # print errors_per_fold_list\n",
    "        # Compute the average cost across all folds for the given lambda\n",
    "\n",
    "    final_lambda_cost = np.mean(errors_per_fold_list)\n",
    "\n",
    "    # Store the cost of the lambda\n",
    "\n",
    "    error_per_lambda.append(final_lambda_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lambda_error_df = pd.DataFrame({\"Alpha/Lambda Value\":lambda_range,\"Mean Error per Fold\": error_per_lambda}).sort(columns = \"Mean Error per Fold\")\n",
    "print lambda_error_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#best_lambda =  0.32  #nbetweenness alpha\n",
    "best_lambda =  10990.74  #betweenness alpha\n",
    "model_final = Lasso(best_lambda)\n",
    "lasso_results = model_final.fit(X_big5_and_network_polyexpanded,y_betweenness)\n",
    "\n",
    "# for i,j in zip(lasso_results.coef_,X_big5_and_network_polyexpanded.columns):    #loop over results\n",
    "#     print (\"Lasso:\", i,\"Coefficient\",j)\n",
    "    \n",
    "lasso_results_df = pd.DataFrame([lasso_results.coef_,X_big5_and_network_polyexpanded.columns]).T\n",
    "lasso_results_df.columns = [\"Coeff. Estimate\",\"Coeff. Name\"]\n",
    "lasso_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for x in list(lasso_results_df[\"Coeff. Name\"]):\n",
    "    print x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lasso_significant_features = lasso_results_df[abs(lasso_results_df[\"Coeff. Estimate\"]) > 0.00]\n",
    "lasso_significant_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now I'm going to re-run a regression using only the variables which Lasso said I should include..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Develop the X matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_lasso_distilled = X_big5_and_network_polyexpanded[list(lasso_significant_features[\"Coeff. Name\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_lasso_distilled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    " X_big5.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_lasso_and_big5 = pd.concat([X_big5, X_lasso_distilled], axis=1, join='inner')\n",
    "X_lasso_and_big5.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Create function which converts model resutls into dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(X_network, y_betweenness, test_size=0.3, random_state=0)\n",
    "model = sm.OLS(y_train,X_train)\n",
    "dictionary_of_model_general_performance_metrics[\"X_lasso_distilled\"] = model_to_dictionary(model,X_test,y_test)\n",
    "results = model.fit()\n",
    "print results.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(X_lasso_and_big5, y_betweenness, test_size=0.3, random_state=0)\n",
    "model = sm.OLS(y_train,X_train)\n",
    "dictionary_of_model_general_performance_metrics[\"X_lasso_distilled\"] = model_to_dictionary(model,X_test,y_test)\n",
    "results = model.fit()\n",
    "print results.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "variance_covariance_matrix = results.cov_HC0\n",
    "variance_covariance_matrix.shape\n",
    "variance_covariance_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here I evaluate my lasso_model against some test data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_hat = results.predict(X_test)\n",
    "X_lasso_distilled_test_ssr = sum((y_hat - y_test)**2)\n",
    "\n",
    "# This is Jeremy's formula\n",
    "X_lasso_distilled_test_RSqAdj_jeremy = 1 - ((sum((y_test-y_hat)**2.0)/(len(X_test) - len(results.params) - 1))\n",
    "            /(sum((y_test - np.mean(y_test))**2.0)/(len(X_test) - 1)))\n",
    "\n",
    "# This is my formula from here: http://onlinestatbook.com/2/effect_size/images/adjusted_rsquared.gif\n",
    "n = len(X_test)\n",
    "p = len(results.params)\n",
    "test_ssr =  X_lasso_distilled_test_ssr\n",
    "test_sst =  sum((y_test - np.mean(y_test))**2.0)\n",
    "\n",
    "print \"test_ssr:\", test_ssr\n",
    "print \"test_sst:\", test_sst\n",
    "\n",
    "\n",
    "test_rsq = 1 - (test_ssr/test_sst)\n",
    "print \"test_rsq:\", test_rsq\n",
    "test_rsq_adj = 1 - (((1 - test_rsq) * (n-1)) / (n-p-1))\n",
    "X_lasso_distilled_test_RSqAdj_zhanna = test_rsq_adj\n",
    "    \n",
    "print \"X_lasso_distilled_test_ssr:\", X_lasso_distilled_test_ssr\n",
    "print \"X_lasso_distilled_test_RSqAdj_zhanna:\", X_lasso_distilled_test_RSqAdj_zhanna\n",
    "print \"X_lasso_distilled_test_RSqAdj_jeremy:\", X_lasso_distilled_test_RSqAdj_jeremy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Clearly my R squared could be either really good or really bad...\n",
    "# IT could be because my sample set is so small..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here I create a function to take the results summary table and turn it into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "peanuts = results_summary_to_dataframe(results)\n",
    "peanuts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now if you filter for only those with a low p-value, and order by coefficient size..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Based on the results above, here are the significant coefficients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "peanuts[peanuts[\"pvals\"]<0.1].sort(columns=\"coeff\", ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now to take the derivatives to plot the graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_lasso_and_big5[\"NETWORKSIZE^1\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "network_range = np.arange(0,1600,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "betas_list = []\n",
    "betas_list_95conf_lower = []\n",
    "betas_list_95conf_higher = []\n",
    "\n",
    "for i in range(0,peanuts.shape[0]):\n",
    "    betas_list.append(peanuts[\"coeff\"][i])\n",
    "    betas_list_95conf_lower.append(peanuts[\"conf_lower\"][i])\n",
    "    betas_list_95conf_higher.append(peanuts[\"conf_higher\"][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Actual Beta Coefficients\n",
    "Chg_Betw_Centrality_from_sExt = betas_list[1] + (betas_list[7] * network_range)\n",
    "Chg_Betw_Centrality_from_sNeu = betas_list[2] + (betas_list[8] * network_range)\n",
    "Chg_Betw_Centrality_from_sAgr = betas_list[3] + (betas_list[9] * network_range)\n",
    "Chg_Betw_Centrality_from_sCon = betas_list[4] + (betas_list[10] * network_range)\n",
    "Chg_Betw_Centrality_from_sOpn = betas_list[5] + (betas_list[11] * network_range)\n",
    "\n",
    "\n",
    "# All lower Estimates - lower lower\n",
    "\n",
    "Chg_Betw_Centrality_from_sExt_low_low = betas_list_95conf_lower[1] + (betas_list_95conf_lower[7] * network_range)\n",
    "Chg_Betw_Centrality_from_sNeu_low_low = betas_list_95conf_lower[2] + (betas_list_95conf_lower[8] * network_range)\n",
    "Chg_Betw_Centrality_from_sAgr_low_low = betas_list_95conf_lower[3] + (betas_list_95conf_lower[9] * network_range)\n",
    "Chg_Betw_Centrality_from_sCon_low_low = betas_list_95conf_lower[4] + (betas_list_95conf_lower[10] * network_range)\n",
    "Chg_Betw_Centrality_from_sOpn_low_low = betas_list_95conf_lower[5] + (betas_list_95conf_lower[11] * network_range)\n",
    "\n",
    "# All higher Estimates - higher higher\n",
    "\n",
    "Chg_Betw_Centrality_from_sExt_high_high = betas_list_95conf_higher[1] + (betas_list_95conf_higher[7] * network_range) \n",
    "Chg_Betw_Centrality_from_sNeu_high_high = betas_list_95conf_higher[2] + (betas_list_95conf_higher[8] * network_range) \n",
    "Chg_Betw_Centrality_from_sAgr_high_high = betas_list_95conf_higher[3] + (betas_list_95conf_higher[9] * network_range) \n",
    "Chg_Betw_Centrality_from_sCon_high_high = betas_list_95conf_higher[4] + (betas_list_95conf_higher[10] * network_range) \n",
    "Chg_Betw_Centrality_from_sOpn_high_high = betas_list_95conf_higher[5] + (betas_list_95conf_higher[11] * network_range) \n",
    "\n",
    "# Lower on first beta, higher for second beta (the one that multiples with network)\n",
    "\n",
    "Chg_Betw_Centrality_from_sExt_low_high = betas_list_95conf_lower[1] + (betas_list_95conf_higher[7] * network_range) \n",
    "Chg_Betw_Centrality_from_sNeu_low_high = betas_list_95conf_lower[2] + (betas_list_95conf_higher[8] * network_range) \n",
    "Chg_Betw_Centrality_from_sAgr_low_high = betas_list_95conf_lower[3] + (betas_list_95conf_higher[9] * network_range) \n",
    "Chg_Betw_Centrality_from_sCon_low_high = betas_list_95conf_lower[4] + (betas_list_95conf_higher[10] * network_range) \n",
    "Chg_Betw_Centrality_from_sOpn_low_high = betas_list_95conf_lower[5] + (betas_list_95conf_higher[11] * network_range) \n",
    "\n",
    "# Higher on first beta, Lower for second beta (the one that multiples with network)\n",
    "\n",
    "Chg_Betw_Centrality_from_sExt_high_low = betas_list_95conf_higher[1] + (betas_list_95conf_lower[7] * network_range)\n",
    "Chg_Betw_Centrality_from_sNeu_high_low = betas_list_95conf_higher[2] + (betas_list_95conf_lower[8] * network_range)\n",
    "Chg_Betw_Centrality_from_sAgr_high_low = betas_list_95conf_higher[3] + (betas_list_95conf_lower[9] * network_range)\n",
    "Chg_Betw_Centrality_from_sCon_high_low = betas_list_95conf_higher[4] + (betas_list_95conf_lower[10] * network_range)\n",
    "Chg_Betw_Centrality_from_sOpn_high_low = betas_list_95conf_higher[5] + (betas_list_95conf_lower[11] * network_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "peanuts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "variance_covariance_matrix_df = pd.DataFrame(variance_covariance_matrix)\n",
    "variance_covariance_matrix_df.columns = list(peanuts.index)\n",
    "variance_covariance_matrix_df.index = list(peanuts.index)\n",
    "variance_covariance_matrix_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This is how to account for the interactionality in creating the 95% confidence intervals.\n",
    "\n",
    "var_sExt = variance_covariance_matrix_df[\"sEXT^1\"][\"sEXT^1\"]\n",
    "cov_sExt_Net = variance_covariance_matrix_df[\"sEXT^1\"][\"sEXT^1 x NETWORKSIZE^1\"]\n",
    "var_sExtNet_sExtNet = variance_covariance_matrix_df[\"sEXT^1 x NETWORKSIZE^1\"][\"sEXT^1 x NETWORKSIZE^1\"]\n",
    "\n",
    "var_sNeu = variance_covariance_matrix_df[\"sNEU^1\"][\"sNEU^1\"]\n",
    "cov_sNeu_Net = variance_covariance_matrix_df[\"sNEU^1\"][\"sNEU^1 x NETWORKSIZE^1\"]\n",
    "var_sNeuNet_sNeuNet = variance_covariance_matrix_df[\"sNEU^1 x NETWORKSIZE^1\"][\"sNEU^1 x NETWORKSIZE^1\"]\n",
    "\n",
    "var_sAgr = variance_covariance_matrix_df[\"sAGR^1\"][\"sAGR^1\"]\n",
    "cov_sAgr_Net = variance_covariance_matrix_df[\"sAGR^1\"][\"sAGR^1 x NETWORKSIZE^1\"]\n",
    "var_sAgrNet_sAgrNet = variance_covariance_matrix_df[\"sAGR^1 x NETWORKSIZE^1\"][\"sAGR^1 x NETWORKSIZE^1\"]\n",
    "\n",
    "var_sCon = variance_covariance_matrix_df[\"sCON^1\"][\"sCON^1\"]\n",
    "cov_sCon_Net = variance_covariance_matrix_df[\"sCON^1\"][\"sCON^1 x NETWORKSIZE^1\"]\n",
    "var_sConNet_sConNet = variance_covariance_matrix_df[\"sCON^1 x NETWORKSIZE^1\"][\"sCON^1 x NETWORKSIZE^1\"]\n",
    "\n",
    "var_sOpn = variance_covariance_matrix_df[\"sOPN^1\"][\"sOPN^1\"]\n",
    "cov_sOpn_Net = variance_covariance_matrix_df[\"sOPN^1\"][\"sOPN^1 x NETWORKSIZE^1\"]\n",
    "var_sOpnNet_sOpnNet = variance_covariance_matrix_df[\"sOPN^1 x NETWORKSIZE^1\"][\"sOPN^1 x NETWORKSIZE^1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "z_95Percent = 1.96\n",
    "\n",
    "Chg_Betw_Centrality_from_sExt_95_plus = Chg_Betw_Centrality_from_sExt + z_95Percent * np.sqrt((var_sExt + (2 * network_range * cov_sExt_Net) + ((network_range ** 2) * var_sExtNet_sExtNet)))\n",
    "Chg_Betw_Centrality_from_sExt_95_minus = Chg_Betw_Centrality_from_sExt - z_95Percent * np.sqrt((var_sExt + (2 * network_range * cov_sExt_Net) + ((network_range ** 2) * var_sExtNet_sExtNet)))\n",
    "\n",
    "Chg_Betw_Centrality_from_sNeu_95_plus = Chg_Betw_Centrality_from_sNeu + z_95Percent * np.sqrt((var_sNeu + (2 * network_range * cov_sNeu_Net) + ((network_range ** 2) * var_sNeuNet_sNeuNet)))\n",
    "Chg_Betw_Centrality_from_sNeu_95_minus = Chg_Betw_Centrality_from_sNeu - z_95Percent * np.sqrt((var_sNeu + (2 * network_range * cov_sNeu_Net) + ((network_range ** 2) * var_sNeuNet_sNeuNet)))\n",
    "\n",
    "Chg_Betw_Centrality_from_sAgr_95_plus = Chg_Betw_Centrality_from_sAgr + z_95Percent * np.sqrt((var_sAgr + (2 * network_range * cov_sAgr_Net) + ((network_range ** 2) * var_sAgrNet_sAgrNet)))\n",
    "Chg_Betw_Centrality_from_sAgr_95_minus = Chg_Betw_Centrality_from_sAgr - z_95Percent * np.sqrt((var_sAgr + (2 * network_range * cov_sAgr_Net) + ((network_range ** 2) * var_sAgrNet_sAgrNet)))\n",
    "\n",
    "Chg_Betw_Centrality_from_sCon_95_plus = Chg_Betw_Centrality_from_sCon + z_95Percent * np.sqrt((var_sCon + (2 * network_range * cov_sCon_Net) + ((network_range ** 2) * var_sConNet_sConNet)))\n",
    "Chg_Betw_Centrality_from_sCon_95_minus = Chg_Betw_Centrality_from_sCon - z_95Percent * np.sqrt((var_sCon + (2 * network_range * cov_sCon_Net) + ((network_range ** 2) * var_sConNet_sConNet)))\n",
    "\n",
    "Chg_Betw_Centrality_from_sOpn_95_plus = Chg_Betw_Centrality_from_sOpn + z_95Percent * np.sqrt((var_sOpn + (2 * network_range * cov_sOpn_Net) + ((network_range ** 2) * var_sOpnNet_sOpnNet)))\n",
    "Chg_Betw_Centrality_from_sOpn_95_minus = Chg_Betw_Centrality_from_sOpn - z_95Percent * np.sqrt((var_sOpn + (2 * network_range * cov_sOpn_Net) + ((network_range ** 2) * var_sOpnNet_sOpnNet)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "marginal_effects_on_betweenness_df = pd.DataFrame({\n",
    "                                                    \"network_range\": network_range,\n",
    "                                                    \"Chg_Betw_Centrality_from_sExt\":Chg_Betw_Centrality_from_sExt,\n",
    "                                                    \"Chg_Betw_Centrality_from_sNeu\":Chg_Betw_Centrality_from_sNeu,\n",
    "                                                    \"Chg_Betw_Centrality_from_sAgr\":Chg_Betw_Centrality_from_sAgr,\n",
    "                                                    \"Chg_Betw_Centrality_from_sCon\":Chg_Betw_Centrality_from_sCon,\n",
    "                                                    \"Chg_Betw_Centrality_from_sOpn\":Chg_Betw_Centrality_from_sOpn,                                        \n",
    "\n",
    "#                                         \"Chg_Betw_Centrality_from_sExt_low_low\":Chg_Betw_Centrality_from_sExt_low_low,\n",
    "#                                         \"Chg_Betw_Centrality_from_sNeu_low_low\":Chg_Betw_Centrality_from_sNeu_low_low,\n",
    "#                                         \"Chg_Betw_Centrality_from_sAgr_low_low\":Chg_Betw_Centrality_from_sAgr_low_low,\n",
    "#                                         \"Chg_Betw_Centrality_from_sCon_low_low\":Chg_Betw_Centrality_from_sCon_low_low,\n",
    "#                                         \"Chg_Betw_Centrality_from_sOpn_low_low\":Chg_Betw_Centrality_from_sOpn_low_low,\n",
    "#                                         \"Chg_Betw_Centrality_from_sExt_high_high\":Chg_Betw_Centrality_from_sExt_high_high,\n",
    "#                                         \"Chg_Betw_Centrality_from_sNeu_high_high\":Chg_Betw_Centrality_from_sNeu_high_high,\n",
    "#                                         \"Chg_Betw_Centrality_from_sAgr_high_high\":Chg_Betw_Centrality_from_sAgr_high_high,\n",
    "#                                         \"Chg_Betw_Centrality_from_sCon_high_high\":Chg_Betw_Centrality_from_sCon_high_high,\n",
    "#                                         \"Chg_Betw_Centrality_from_sOpn_high_high\":Chg_Betw_Centrality_from_sOpn_high_high,\n",
    "#                                         \"Chg_Betw_Centrality_from_sExt_low_high \":Chg_Betw_Centrality_from_sExt_low_high ,\n",
    "#                                         \"Chg_Betw_Centrality_from_sNeu_low_high \":Chg_Betw_Centrality_from_sNeu_low_high ,\n",
    "#                                         \"Chg_Betw_Centrality_from_sAgr_low_high \":Chg_Betw_Centrality_from_sAgr_low_high ,\n",
    "#                                         \"Chg_Betw_Centrality_from_sCon_low_high \":Chg_Betw_Centrality_from_sCon_low_high ,\n",
    "#                                         \"Chg_Betw_Centrality_from_sOpn_low_high \":Chg_Betw_Centrality_from_sOpn_low_high ,\n",
    "#                                         \"Chg_Betw_Centrality_from_sExt_high_low \":Chg_Betw_Centrality_from_sExt_high_low ,\n",
    "#                                         \"Chg_Betw_Centrality_from_sNeu_high_low \":Chg_Betw_Centrality_from_sNeu_high_low ,\n",
    "#                                         \"Chg_Betw_Centrality_from_sAgr_high_low \":Chg_Betw_Centrality_from_sAgr_high_low ,\n",
    "#                                         \"Chg_Betw_Centrality_from_sCon_high_low \":Chg_Betw_Centrality_from_sCon_high_low ,\n",
    "#                                         \"Chg_Betw_Centrality_from_sOpn_high_low \":Chg_Betw_Centrality_from_sOpn_high_low \n",
    "    \n",
    "    \n",
    "    \n",
    "    })\n",
    "\n",
    "marginal_effects_on_betweenness_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import graphlab as gl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sf_marginal_effects_on_betweenness_df = gl.SFrame(data=marginal_effects_on_betweenness_df)\n",
    "sf_marginal_effects_on_betweenness_df.show()\n",
    "\n",
    "#https://dato.com/products/create/docs/graphlab.canvas.html\n",
    "gl.canvas.set_target('ipynb')\n",
    "#gl.canvas.set_target('browser')\n",
    "#show(view=\"Summary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sf_marginal_effects_on_betweenness_df.show(view=\"Summary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# https://dato.com/products/create/docs/generated/graphlab.SFrame.show.html\n",
    "sf_marginal_effects_on_betweenness_df.show(view=\"Scatter Plot\", x=\"network_range\", y=\"Chg_Betw_Centrality_from_sAgr\")\n",
    "sf_marginal_effects_on_betweenness_df.show(view=\"Scatter Plot\", x=\"network_range\", y=\"Chg_Betw_Centrality_from_sCon\")\n",
    "sf_marginal_effects_on_betweenness_df.show(view=\"Scatter Plot\", x=\"network_range\", y=\"Chg_Betw_Centrality_from_sExt\")\n",
    "sf_marginal_effects_on_betweenness_df.show(view=\"Scatter Plot\", x=\"network_range\", y=\"Chg_Betw_Centrality_from_sNeu\")\n",
    "sf_marginal_effects_on_betweenness_df.show(view=\"Scatter Plot\", x=\"network_range\", y=\"Chg_Betw_Centrality_from_sOpn\")\n",
    "# Other useful: https://dato.com/learn/userguide/timeseries/timeseries-data.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt # side-stepping mpl backend\n",
    "import matplotlib.gridspec as gridspec # subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import plotly.plotly as py\n",
    "from plotly.graph_objs import *\n",
    "import plotly.tools as tls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doing the charts for sAgr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig3 = plt.figure()\n",
    "plt.figure(figsize = (20,10))\n",
    "# Make a legend for specific lines.\n",
    "\n",
    "# note that plot returns a list of lines.  The \"l1, = plot\" usage\n",
    "# extracts the first element of the list into l1 using tuple\n",
    "# unpacking.  So l1 is a Line2D instance, not a sequence of lines\n",
    "l1, = plt.plot(network_range, y__sAgr,'rs-.')\n",
    "l2, = plt.plot(np.zeros(len(network_range)))     # Adding the baseline at zero\n",
    "\n",
    "l_95_minus, = plt.plot(network_range,Chg_Betw_Centrality_from_sAgr_95_minus)\n",
    "l_95_plus,= plt.plot(network_range,Chg_Betw_Centrality_from_sAgr_95_plus)\n",
    "\n",
    "#l4, = plt.plot(x, np.exp(-x) * np.sin(2 * np.pi * x), 'rs-.')\n",
    "\n",
    "plt.xlabel('Network')\n",
    "plt.ylabel('Ch. in Bet. Centrality')\n",
    "plt.title('Change in Betweenness Centrality due to sAgr as Network Changes')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig3 = plt.figure()\n",
    "plt.figure(figsize = (20,10))\n",
    "# Make a legend for specific lines.\n",
    "\n",
    "# note that plot returns a list of lines.  The \"l1, = plot\" usage\n",
    "# extracts the first element of the list into l1 using tuple\n",
    "# unpacking.  So l1 is a Line2D instance, not a sequence of lines\n",
    "l1, = plt.plot(network_range, y__sCon,'rs-.')\n",
    "l2, = plt.plot(np.zeros(len(network_range)))     # Adding the baseline at zero\n",
    "\n",
    "l_95_minus, = plt.plot(network_range,Chg_Betw_Centrality_from_sCon_95_minus)\n",
    "l_95_plus,= plt.plot(network_range,Chg_Betw_Centrality_from_sCon_95_plus)\n",
    "\n",
    "plt.xlabel('Network')\n",
    "plt.ylabel('Ch. in Bet. Centrality')\n",
    "plt.title('Change in Betweenness Centrality due to sCon as Network Changes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "fig3 = plt.figure()\n",
    "plt.figure(figsize = (20,10))\n",
    "# Make a legend for specific lines.\n",
    "\n",
    "# note that plot returns a list of lines.  The \"l1, = plot\" usage\n",
    "# extracts the first element of the list into l1 using tuple\n",
    "# unpacking.  So l1 is a Line2D instance, not a sequence of lines\n",
    "l1, = plt.plot(network_range, y__sExt,'rs-.')\n",
    "l2, = plt.plot(np.zeros(len(network_range)))     # Adding the baseline at zero\n",
    "\n",
    "\n",
    "l_95_minus, = plt.plot(network_range,Chg_Betw_Centrality_from_sExt_95_minus)\n",
    "l_95_plus,= plt.plot(network_range,Chg_Betw_Centrality_from_sExt_95_plus)\n",
    "\n",
    "\n",
    "plt.xlabel('Network')\n",
    "plt.ylabel('Ch. in Bet. Centrality')\n",
    "plt.title('Change in Betweenness Centrality due to sExt as Network Changes')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig3 = plt.figure()\n",
    "plt.figure(figsize = (20,10))\n",
    "# Make a legend for specific lines.\n",
    "\n",
    "# note that plot returns a list of lines.  The \"l1, = plot\" usage\n",
    "# extracts the first element of the list into l1 using tuple\n",
    "# unpacking.  So l1 is a Line2D instance, not a sequence of lines\n",
    "l1, = plt.plot(network_range, y__sNeu,'rs-.')\n",
    "l2, = plt.plot(np.zeros(len(network_range)))     # Adding the baseline at zero\n",
    "\n",
    "l_95_minus, = plt.plot(network_range,Chg_Betw_Centrality_from_sNeu_95_minus)\n",
    "l_95_plus,= plt.plot(network_range,Chg_Betw_Centrality_from_sNeu_95_plus)\n",
    "\n",
    "plt.xlabel('Network')\n",
    "plt.ylabel('Ch. in Bet. Centrality')\n",
    "plt.title('Change in Betweenness Centrality due to sNeu as Network Changes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "fig3 = plt.figure()\n",
    "plt.figure(figsize = (20,10))\n",
    "# Make a legend for specific lines.\n",
    "\n",
    "# note that plot returns a list of lines.  The \"l1, = plot\" usage\n",
    "# extracts the first element of the list into l1 using tuple\n",
    "# unpacking.  So l1 is a Line2D instance, not a sequence of lines\n",
    "l1, = plt.plot(network_range, y__sOpn,'rs-.')\n",
    "l2, = plt.plot(np.zeros(len(network_range)))     # Adding the baseline at zero\n",
    "\n",
    "\n",
    "l_95_minus, = plt.plot(network_range,Chg_Betw_Centrality_from_sOpn_95_minus)\n",
    "l_95_plus,= plt.plot(network_range,Chg_Betw_Centrality_from_sOpn_95_plus)\n",
    "\n",
    "\n",
    "plt.xlabel('Network')\n",
    "plt.ylabel('Ch. in Bet. Centrality')\n",
    "plt.title('Change in Betweenness Centrality due to sOpn as Network Changes')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_big5_and_network.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reflection on results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find the biggest \n",
    "\n",
    "Neuroticism and Openess Cobined -- Tend to greatly inccrease\n",
    "\n",
    "Extroversion & Conscientiousnes Combined --- interesting that in one version the effect is positive, but too much extroversion and conscientiousness leads to negative effect. \n",
    "\n",
    "Aggreableness -- Tends to increase the value...but why to the 4th power?\n",
    "\n",
    "What's with the (Aggreableness + Openness) combo and why do they tend to make negative results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Against the CV set, compare the cost functions for each of the models and choose which model is the best (predict... and then get the difference) + ALso compare the adjusted R squared for all of them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Once the model is chosen, retrain that model using the combination of CV + Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Test your final model against your test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Diagnostic Tool #1:\n",
    "\n",
    "https://theclevermachine.files.wordpress.com/2013/04/bias-variance-train-test-error.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Diagnostic Tool #2\n",
    "### Step 7 - If you increase the size of your training set (from 0 to the full amount)\n",
    "http://www.bigdataexaminer.com/wp-content/uploads/2014/11/code-9.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis & Interpretations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Add sentiment analysis stuff\n",
    "# Create Repository called luther (?)\n",
    "\n",
    "# Extract out #auth & status updates\n",
    "\n",
    "# Generate some \"word usage\" variable per person (collapse all statuses into a single metric per person)\n",
    "\n",
    "# Add that metric to your model\n",
    "# See if that's representative for the centrality metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#) Identify the profile for the \"lowest\" network users based on profile. and highest -- display the two types of profiles created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Next Steps:\n",
    "    - K means of dataset/users / networks\n",
    "    - Sentiment analysis as an additional feature (word2vec?)\n",
    "    - Considering alternate network theory variables (e.g. centrality, etc...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import graphlab as gl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#df = pandas.DataFrame()\n",
    "sf_grouped = gl.SFrame(data=df_grouped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sf_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "8399.66/86.33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "8096.6/92.24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "18123.1 / 97.81 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sf_grouped.show()\n",
    "gl.canvas.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Ask Jeremey\n",
    "-- Woudl it make sense to divide betweenness by network size?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
